 

//기초설명
하둡 = 분산컴퓨팅플랫폼 /스토리지 분산파일시스템으로 HDFS사용 /데이터를 블록형태로 저장
맵단계(입력리스트가 HDFS에 블록으로 저장됨) 
-> 리듀스단계(맵의 출력리스트가 리듀스의 입력리스트로 연결 /입력리스트를 출력값으로 병합함(리듀스))
맵리듀스는 대규모 데이터 일괄처리에는 뒤어나지만 반복연산, 저지연등에는 효과적이지못함

스파크는 데이터를 분산메모리로 캐싱할수있어 빠른연산, 저지연응답속도에 유리 ################################
스파크 RDD가 제공하는 연산자는 변환, 행동으로 나뉨 
/변환연산자 연속사용시 DAG형태로 기억해뒀다가 행동연산자가 사용된시점에 변환연산자와 행동연산자를 빠르게 처리

============================================
아파치스쿱-하둡과 정형데이터베이스간 벌크데이터전송 (mysql) ################################
아파치플럼-대용량로그데이터를 수집해 HDFS로 전송하는 분산서비스 ################################
피그-하둡의 추출,변환,적재(ETL)을 쉽게구현 ################################

아파치하이브-SQL로 하둡데이터분석/ openNLP자연어처리함수를 하이브UDF로 추가시 데이터분석가능
스파크- 대화형데이터처리기능 /데이터전처리 효과적/ RDD기반 데이터프레임 사용 ###############################
스파크SQL -하이브대체하는 분산SQL처리엔진 / 대규모데이터셋 처리 관계형대수연산 API인 Dataframe제공 ################################
스파크MLlib-분산데이터셋에 적용할수잇는 머신러닝라이브러리 ################################

R환경에서 하둡과연동가능 /R Hadoop -R을통해 HDFS파일접근 / RMR -맵리듀스실행 
/R Plyr -HDFS데이터가공 / RODBC -R콘솔에서 하이브접속

하둡데이터레이크 -기존의 데이터웨어하우스에비해 이점 (기존엔 ETL단계에서폐기하는경우많음/ 모든데이터를 저장하는 중앙스토리지)
어플리케이션이 데이터처리시점에 이르러서야 ETL 데이터변환과정함 (스키마온리드)
기존의 데이터웨어하우스는 스키마온라이트 -데이터가 어떻게 활용될지 사전에 추측하는 설계단계 거침

================================================= 
4장  데이터입수 (기초코드)############################# 100p정리

//파일/csv파일을 hdfs로 전송 /하이브테이블로 가져오기   
practical-data-science-with-hadoop-and-spark-master/ch04/Hive-Import-CSV/names.csv  에서 csv 다운받고 hdfs에 경로만들고 csv파일을넣음 ##################
hdfs dfs -kdir names
hdfs dfs -put names.csv names 
Names_txt라는 테이블 만듬 

ORC기반테이블(Names) 만들고 이전테이블을 옮김 ??
조건절걸고 경로기반 테이블(Names_text)를 Names_part에 옮김 ??

//pySpark 스파크의 파이썬API 로 기본적 데이터 입수 
//스파크를 사용해 데이터를 하이브테이블로 가져오기  -Dataframe / spark.apache.org 참조  
csv파일 읽어 csv_data에 저장하고, csv_data.map(lambda p:p.split(",") 으로 RDD생성
toDF()함수로 csv_data.map  /toDF()함수로 csv_data RDD의 데이터를 df_csv라는 스파크SQL DataFrame에 저장함 ##################
df_csv를 saveAsTable로 하이브 테이블 employees에에 저장함 

//소스코드
https://github.com/LIMSCODE/006811/blob/master/practical-data-science-with-hadoop-and-spark-master/ch04/Spark-Import-CSV/NOTES_Spark-Import_HIVE-CSV-JSON.txt


=================================================
5장 데이터개조 (볼필요없음)
데이터 크기가 테라바이트가 되니까 대규모데이터를 메모리에서 처리불가 - 하둡사용이유 (피그,하이브,스파크 개조작업시 이용) #####################
데이터의 개조란, 데이터의품질 교정 / 원시데이터를 특징행렬로 변환 하는것이고 이것이 어렵다 #######################
CMS 메디컬 서비스센터 데이터로 가공할것  
www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier2013.html
HDFS내 디렉터리생성 / 여기에 파일복사 / 하이브 명령셸시작하고 외부테이블만듬

피그 하이브 스파크
pig사용법 및 코드 150p ########################  
피그-하둡의 추출,변환,적재(ETL)을 쉽게구현
 

================================================= 
8장 (프로젝트에넣음####쉬움)
하둡,스파크 활용해 대규모 트윗데이터의 감성분석을 위한 분류모델 

1. 기초작업
sentiment데이터를 웹에서 입수후 mkdir로 저장 /curl명령어로 sentiment.zip을 웹에서 다운받음, 하둡에저장
긍정/부정 단어목록인 wordlist를 mkdir로 저장
tweets_raw 테이블, sentiment_words 테이블 만듬  ######################## 

2. 특징변수 생성쿼리에 사용할 pySpark UDF (user defined function) 함수 만듬 ######################## 
텍스트를 단어로 분할하는 UDF인 토크나이저(def tok_str)
감성점수 저장한 테이블 입수하고 그안의  wordlist로 단어리스트로부터 감성점수 계산하는 pyspark UDF 정의 ( def pscore, def nscore )  
필요 UDF 함수 준비되었으니, 스파크 SQL로 특징행렬생성

3. 스파크 Dataframe API 사용하여 스파크SQL로 모델에 사용할 특징행렬을 생성하고, 결과를 임시테이블인 fm에저장
hc = HiveContext(SparkContext sc) 	//초기설정 hc

wv = hc.table('sentiment_words').collect()  ######################## 
tw1 = hc.sql("""   //스파크sql코드로 특징행령(select값)생성 ######################## 
SELECT text, query, polarity, date,
       regexp_extract(date, '([0-9]{2}):([0-9]{2}):([0-9]{2})', 1) as hour,
       regexp_extract(date, '(Sun|Mon|Tue|Wed|Thu|Fri|Sat)', 1) as dayofweek,
       regexp_extract(date, '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)', 1) as month
FROM tweets 
""")	

tw2, tw3 = tw1가공
tw3.registerTempTable("fm") //결과로생성된 특징행렬을 스파크SQL 임시테이블인 fm에저장함 ######################## 


4. 스파크ML의 파이프라인API로 머신러닝 파이프라인 구성, 분류모델 학습
분류모델 학습 (파이프라인구성, 파이프라인.fit으로 모델학습)  ######################## 
 
머신러닝 파이프라인구성
inx1 = StringIndexer
rf = RandomForestClassifier
p = Pipeline(stage = [inx1 ,2, 3, 4, hashingTF, idf ,va, rf])
(trainSet, testSet) = hc.table("fm").randomSplit([0.7,0.3]) ######################## 
model = p.fit(trainData)     ######################## pipeline의  fit으로 모델학습시킴

5. 정확도 평가함수인 def eval(lap) 에 넣어서 정밀도, 재현율, 정확도 평가
results = model.transform(testData)     ######################## 학습시킨 모델로 예측한다
lap = results.selet("label", "prediction")
m = eval_metrics(lap) 
print m
 
6. 예측수행

//8장 소스코드
https://github.com/LIMSCODE/006811/blob/master/practical-data-science-with-hadoop-and-spark-master/ch08/script.py




======================================================
======================================================
======================================================
======================================================
======================================================
======================================================
9장 (볼필요없음,,난이도있음)
LDA를 활용한 주제모델링 #############
데이터입수 HDFS에저장
특징변수생성
각무서 읽어들여 토큰화
LDA생성 -상위5개추출 -MLlib실행해 주제5개도출
/LDA실행이 끝나면 발견된주제모델이 ladModel에저장됨 
/주제별 가장중요한단어가 출력됨 -상위키워드5개가 각주제를 잘표현함, 해당주제의 적절한 의미 전달함

 
LDA를 활용한 주제모델링 #############

OSHSUMED데이터셋을 내려받아 hdfs에저장
특징변수생성- 토큰화(openNLP사용) / openNLP는 하둡과 별개이므로 sc.addJar로 스파크에 추가해야함 ###

불용어목록을 가져온후, 
var tokenized :RDD ~

토큰화한결과를 RDD로 만들어 캐시에 저장한다. /단어수를 제한
var wordounts : RDD

TF-IDF인코딩으로 문서를 특징변수의 벡터로 표현한다
-결과를담은 tfidfDocs ###는 (id, vec)의 키-값쌍을 가진 RDD이다   //vec은 문서의 TD-IDF특징변수 표현하는 희소벡터 ,id는 문서고유id

//LDA실행
MLlib실행해 데이터셋에서 주제 5개 도출-주제모델이 ldaModel에 저장됨
val lda = new LDA()
val ldaModel = lda.run(tfidDocs)   #####
LDA실행끝나면 발견된 주제모델이 ldaModel에 저장됨 
ldaModel.describeTopic  #####최종적으로 사용하는 함수 ##########-for문돌려출력

//결과
ldaModel에 저장된 주제별로 가장중요한 단어 상위5개
1 감염 hiv patient
2 tumor cell cancer
3,4,5 
---> 각행의 주제가 1번 -HIV , 2번-암 , 3번-,...... 임을 유추할수있다 #############

TF-IDF를통해 벡터화함으로써 describeTopic으로 주제별로 가장중요한단어 5개를 추출할수있다... ###########
=====
하둡 fs코드
practical-data-science-with-hadoop-and-spark-master/ch09/ingest.sh
스탑워드
practical-data-science-with-hadoop-and-spark-master/ch09/stop-words.txt
파이썬코드
practical-data-science-with-hadoop-and-spark-master/ch09/lda-script.scala

======================================================

11장
하둡의 자연어처리도구 (볼필요없음)

